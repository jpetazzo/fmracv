default parameters: image_size=224x224, kernel_size=5x5

batch_size=32, channels=1
Epoch 10/10
65/65 [==============================] - 4s 62ms/step - loss: 0.0493 - accuracy: 0.9726 - val_loss: 0.1246 - val_accuracy: 0.9408

batch_size=32, channels=3
65/65 [==============================] - 4s 69ms/step - loss: 0.0575 - accuracy: 0.9735 - val_loss: 0.1200 - val_accuracy: 0.9538

batch_size=64, channels=3
33/33 [==============================] - 4s 131ms/step - loss: 0.0641 - accuracy: 0.9673 - val_loss: 0.1241 - val_accuracy: 0.9422

batch_size=32, channels=3, image_size=160x96
65/65 [==============================] - 1s 23ms/step - loss: 0.1035 - accuracy: 0.9456 - val_loss: 0.1762 - val_accuracy: 0.9061

batch_size=32, channels=3, image_size=294x184
65/65 [==============================] - 5s 76ms/step - loss: 0.0546 - accuracy: 0.9692 - val_loss: 0.1456 - val_accuracy: 0.9436

batch_size=32, channels=3, image_size=224x224, kernel_size=5x5
65/65 [==============================] - 7s 100ms/step - loss: 0.1278 - accuracy: 0.9298 - val_loss: 0.1620 - val_accuracy: 0.9133

batch_size=32, channels=3, image_size=448x448
65/65 [==============================] - 10s 148ms/step - loss: 0.0251 - accuracy: 0.9885 - val_loss: 0.1755 - val_accuracy: 0.9379

batch_size=32, channels=3, image_size=448x448, epochs=20
65/65 [==============================] - 10s 149ms/step - loss: 6.3805e-05 - accuracy: 1.0000 - val_loss: 0.3883 - val_accuracy: 0.9422

batch_size=32, channels=1, image_size=224x224, epochs=15
65/65 [==============================] - 2s 35ms/step - loss: 0.0231 - accuracy: 0.9870 - val_loss: 0.2604 - val_accuracy: 0.9306

batch_size=32, channels=1, image_size=224x224, epochs=15, dense layer with 128 nodes
65/65 [==============================] - 2s 32ms/step - loss: 0.0184 - accuracy: 0.9933 - val_loss: 0.1883 - val_accuracy: 0.9436

batch_size=32, channels=3, image_size=224x224, epochs=15, dense layer with 128 nodes
65/65 [==============================] - 2s 36ms/step - loss: 0.0052 - accuracy: 0.9986 - val_loss: 0.1848 - val_accuracy: 0.9408

batch_size=64, channels=3, image_size=224x224, epochs=15, dense layer with 128 nodes
33/33 [==============================] - 2s 66ms/step - loss: 0.0386 - accuracy: 0.9808 - val_loss: 0.1394 - val_accuracy: 0.9408

batch_size=128, channels=3, image_size=224x224, epochs=15, dense layer with 128 nodes
17/17 [==============================] - 2s 122ms/step - loss: 0.0879 - accuracy: 0.9529 - val_loss: 0.1674 - val_accuracy: 0.9220

batch_size=128, channels=3, image_size=224x224, epochs=15
17/17 [==============================] - 2s 134ms/step - loss: 0.0830 - accuracy: 0.9490 - val_loss: 0.1325 - val_accuracy: 0.9393

batch_size=128, channels=3, image_size=224x224, epochs=15, two classes only
16/16 [==============================] - 2s 136ms/step - loss: 0.0287 - accuracy: 0.9914 - val_loss: 0.0428 - val_accuracy: 0.9802

avec un plus gros dataset de training
112/112 [==============================] - 4s 39ms/step - loss: 0.0162 - accuracy: 0.9933 - val_loss: 0.1337 - val_accuracy: 0.9708

even bigger data set
160/160 [==============================] - 6s 38ms/step - loss: 0.0026 - accuracy: 0.9988 - val_loss: 0.1372 - val_accuracy: 0.9775

two dense layers with 256 nodes
160/160 [==============================] - 6s 38ms/step - loss: 0.0015 - accuracy: 0.9992 - val_loss: 0.1160 - val_accuracy: 0.9815

three dense layers with 256 nodes
160/160 [==============================] - 6s 38ms/step - loss: 0.0070 - accuracy: 0.9967 - val_loss: 0.0946 - val_accuracy: 0.9730

double all conv layers; two dense layers
160/160 [==============================] - 12s 78ms/step - loss: 0.0119 - accuracy: 0.9928 - val_loss: 0.0908 - val_accuracy: 0.9696

add BatchNormalization between the two dense layers: didn't work (val_accuracy kept jumping from 0.32 to 0.65 and back)

add Dropout(0.5) between the two dense layers: didn't work (accuracy stuck around 0.65)

double all conv layers; two dense layers; Dropout(0.1) just before the output layer
160/160 [==============================] - 12s 78ms/step - loss: 0.0192 - accuracy: 0.9904 - val_loss: 0.1005 - val_accuracy: 0.9663

double all conv layers; two dense layers; Dropout(0.1) just before the output layer; categorical_crossentropy
160/160 [==============================] - 12s 78ms/step - loss: 0.0264 - accuracy: 0.9887 - val_loss: 0.1936 - val_accuracy: 0.9567

double all conv layers; two dense layers; categorical_crossentropy; lr=0.01
160/160 [==============================] - 13s 78ms/step - loss: 0.0307 - accuracy: 0.9900 - val_loss: 0.1163 - val_accuracy: 0.9764

double all conv layers; two dense layers; categorical_crossentropy; lr=0.0001
160/160 [==============================] - 12s 78ms/step - loss: 0.0641 - accuracy: 0.9793 - val_loss: 0.0956 - val_accuracy: 0.9736

double all conv layers; two dense layers; categorical_crossentropy; adagrad optimizer
160/160 [==============================] - 13s 79ms/step - loss: 0.1756 - accuracy: 0.9467 - val_loss: 0.1511 - val_accuracy: 0.9500

two dense layers, categorical_crossentropy
160/160 [==============================] - 6s 39ms/step - loss: 0.0273 - accuracy: 0.9904 - val_loss: 0.1586 - val_accuracy: 0.9764

batch_size=16, two dense layers, categorical_crossentropy

dataset: 13k images three classes
336/336 [==============================] - 21s 63ms/step - loss: 0.0251 - accuracy: 0.9919 - val_loss: 0.1802 - val_accuracy: 0.9613

dataset: 13k images two classes
328/328 [==============================] - 20s 61ms/step - loss: 0.0182 - accuracy: 0.9940 - val_loss: 0.0398 - val_accuracy: 0.9851

dataset: 13k images two classes, get lucky
328/328 [==============================] - 20s 62ms/step - loss: 0.0114 - accuracy: 0.9963 - val_loss: 0.0285 - val_accuracy: 0.9950

dataset: 13k images two classes, remove one aberration
328/328 [==============================] - 20s 61ms/step - loss: 0.0194 - accuracy: 0.9933 - val_loss: 0.0595 - val_accuracy: 0.9870
